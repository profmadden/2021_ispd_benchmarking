% The first command in your LaTeX source must be the \documentclass command.


\documentclass[sigconf]{acmart}
\settopmatter{printacmref=true}

\fancyhead{}
  % do not delete this code.

\usepackage{balance}
  % for creating a balanced last page (usually last page with references)

% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
% Rights management information. 
% This information is sent to you when you complete the rights form.
% These commands have SAMPLE values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you complete the rights form.
%
% These commands are for a PROCEEDINGS abstract or paper.


% Submission ID. 
% Use this when submitting an article to a sponsored event. You'll receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{123-A56-BU3}


% end of the preamble, start of the body of the document source.


\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{epsfig,url}
\usepackage{epsf}

%% Added by JJ
\usepackage{cleveref}
\usepackage{tabularx}
\usepackage{ragged2e}
%%

%Control space between caption and figure/table
\captionsetup[figure]{font=small,skip=5pt}
%\captionsetup[table]{font=small,skip=0pt}


\copyrightyear{2021}
\acmYear{2021}
\setcopyright{acmcopyright}\acmConference[ISPD '21]{Proceedings of the 2021 International Symposium on Physical Design}{March 22--24, 2021}{Virtual Event, USA}
\acmBooktitle{Proceedings of the 2021 International Symposium on Physical Design (ISPD '21), March 22--24, 2021, Virtual Event, USA}
\acmPrice{15.00}
\acmDOI{10.1145/3439706.3446885}
\acmISBN{978-1-4503-8300-4/21/03}

\begin{document}


\title{Still Benchmarking After All These Years\\(Draft, Invited)}
\iffalse
\author{Blind Review}
\else
\author{Ismail S. K. Bustany}
\email{ISMAILB@xilinx.com}
\affiliation{
  \institution{Xilinx Inc.}
  \city{Fremont}
  \state{California}
}
\author{Jinwook Jung}
\email{jinwookjung@ibm.com}
\affiliation{
  \institution{IBM T.J.\ Watson Research Center}
%  \streetaddress{1101 Kitchawan Rd}
  \city{Yorktown Heights}
  \state{New York}
%  \postcode{10598}
}
\author{Patrick H. Madden}
\email{pmadden@binghamton.edu}
\affiliation{
  \institution{SUNY Binghamton CSD}
%  \streetaddress{Box 6000}
  \city{Binghamton}
  \state{New York}
%  \postcode{13902}
}
\author{Natarajan Viswanathan}
\email{nviswan@cadence.com}
\affiliation{
  \institution{Cadence Design Systems, Inc.}
  \city{Austin}
  \state{Texas}
}
\author{Stephen Yang}
\email{stepheny@xilinx.com}
\affiliation{
  \institution{Xilinx Inc.}
  \city{Fremont}
  \state{California}
}


 
\fi
\begin{abstract}
\input{abstract.tex}
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010583.10010682</concept_id>
       <concept_desc>Hardware~Electronic design automation</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Hardware~Electronic design automation}


%
% Keywords. The author(s) should pick words that accurately describe the work being
% presented. Separate the keywords with commas.

\keywords{benchmarking; circuit placement; metrics}

\maketitle

\section{Introduction}

Integrated circuits have evolved at a breakneck pace for decades.  The
first MOSFET transistors were created at the end of the 1950's, with
Moore\cite{Moore650114} making his bold prediction for exponential
growth in 1965.  While it might have seemed far-fetched or wildly
optimistic at the time, Moore's Law has been a relentless juggernaut.
In 1960, only a handful of transistors could be integrated into a
single device, but by 1980, just twenty years later, the Motorola
68000 had roughly sixty-eight thousand transistors.  Twenty years
after that, the Intel Pentium 4 featured roughly forty-two million
transistors.  Twenty years after that, in the era in which this paper
is being written, the Apple M1 processor has in excess of sixteen
billion transistors.

The technological advances required for this progress
have not been spontaneous.  At each
step along the way, researchers from both academia and
industry, and from a wide range of fields,
have been quietly identifying and overcoming
technological barriers.  The massive electronics industry,
which has impacted almost every aspect of modern life,
is the product of decades of hard work.

\iffalse
VLSI physical design focuses on translating the logical function
of a system into an equivalent physical representation.  
Logic synthesis involves translation of higher level system
specifications into simple logic elements.  This is followed by
placement, the location of logic elements onto a two-dimensional
plane, and then routing, which connects logic elements together
with wiring.  This portion of the circuit design process is
normally referred to as ``physical design.''
\fi

Benchmarking, the practice of using example problems to compare
different solution approaches, has been critical to this progress.
Benchmarks with well defined metrics, and that are widely
available, are most helpful; they can engage a broad audience, drawing
in contributions and ideas.  The market forces that have driven the
electronics industry forward have also in some ways been a barrier;
secrecy
around process technologies, circuit designs, and design methodologies
give a competitive advantage, and groups at the leading edge of
technology are often reluctant to reveal their secrets.

In this paper, we look back on benchmarking in physical design, with
an emphasis on circuit placement.  Following something of a rocky
start, the physical design community has found workable solutions that
protect the intellectual property of industry leaders, while also
opening up design benchmarks for the general public.  This has allowed
physical design to evolve to a state where sustained, regular progress
can be made.  The annual ISPD contests have been a catalyst for
research, and have also served as an excellent way strengthen and grow
the research community.

% https://en.wikipedia.org/wiki/Transistor_count
% Better citation for stuff like this?
\iffalse
{\bf How to say this?
  Economic driver pushing technology forward.  Lots of
  companies competing for market share.  Secrecy on
  design technology, circuit designs, gives a competitive
  advantage -- but also limits the number of people who
  can solve the problems that need to be solved.

  For academic researchers in particular to be helpful,
  details of the problems needing solving have to be
  available.  Without test cases, and things that are
  suitable for publication, they cannot assist.
}




Making progress in design automation has been challenging;
the problems addressed are fundamentally hard from a scientific
perspective. Complicating matters further are the financial
implications -- semiconductor manufacturers, design tool companies,
and circuit designers, are all interested in making profits, and
this depends on some degree of secrecy in their work.  At the
same time, solving design problems requires assistance from
others.
\fi


\section{Placement Benchmarking}

The very earliest circuits were designed by hand, each one a custom
creation. As design sizes increased, and semiconductor fabrication
technologies became more refined, automation became essential and
designers converged on a few common conventions and
abstractions. Lambda-based design rules and standard cell
libraries\cite{Mead93} allowed the physical design of circuits to be
handled by software.  A design flow of logic synthesis, to placement,
and to routing, enabled interchangable design automation tools and
encouraged common design file formats.

In circuit placement, there are a number of different metrics
used to evaluate solutions.  The simplest is a {\em half perimeter
  wire length estimate}; for a standard cell design, the
cells are placed into horizontal rows, with the bounding boxes
of the pins for each net being used as an estimate of routing
distances.  With the simple metric, determining which placement
is ``best'' can be done by simply comparing total lengths.

Circuit routing demands can complicate matters considerably, however.
If the wiring demand in an area of the chip is high, this can
result in routing detours -- which increase wire lengths -- or
even result in routing failure.  To address this, placement
metrics often include a {\em density} constraint -- limiting the
number of cells in any area of the design, or the total number of
pins.

The metrics can be improved further, taking into account the
wiring produced by a complete run of a global and detail router.
With information about cell functionality, circuit timing, power
consumption, and other metrics can be evaluated.

There is something of a balancing act with placement metrics.
A simple metric such as wire length can give a clear indication
of ``winners'' and ``losers'' -- but this may miss key performance
concerns.  If multiple metrics are in place, which approach
can be considered ``best'' is much less clear.


\iffalse
% \subsection{Metrics}
{\bf Simple Placement Metrics}

As the new benchmarks did not contain functional information,
timing models, and so forth, it was entirely practical to
simplify the file formats so that they were easy to parse.
Much of this work was done as part of the of the
GSRC ``bookshelf''\cite{umichbookshelf}, with many groups
contributing software tools and benchmark problems.

{\bf Complex Placement Metrics}
\fi


\subsection{Early Benchmarks}


There were few commonly available circuits and benchmark problems in
the early days.  In 1987, a panel discussion\cite{Preas87} highlighted
the need for good benchmarks for standard cell design, and a few years
later in 1991, the MCNC benchmarks\cite{Kozminski91} were presented
and quickly gained prominence.  The benchmarks are shown in Table
\ref{tab:mcnc} -- while they were not comparable in size and
complexity to the leading edge of industrial design, they filled a
vacuum, and were a catalyst for academic research.  The availability
of circuits that could be used as physical design benchmarks sparked a
wave of circuit placement research; there was heavy competition
between simulated annealing\cite{Kirkpatrick830671}, numerical methods\cite{Kleinhans910356}, and recursive
bisection\cite{Dunlop850092}.

\begin{table}
  \caption{The original MCNC standard cell
    benchmarks\cite{Kozminski91}.}\label{tab:mcnc}
  \begin{tabular}{|l|r|r|r|} \hline
Name & \# cells & \# nets & \# I/O \\ \hline
    fract   &   125 &   147 & 24 \\ \hline
   primary1 &   752 &   985 & 81 \\ \hline
    struct  &  1888 &  1920 & 64 \\ \hline
  industry1 &  2271 & 2593 & 814 \\ \hline
  primary2 & 2907 & 3136 & 107 \\ \hline
  biomed & 6417 & 5742 & 97 \\ \hline
  industry2 & 12142 & 13419 & 495 \\ \hline
  industry3 & 15059 & 21940 & 375 \\ \hline
  \end{tabular}
\end{table}


There was hope that more benchmarks would
become available, with Kozminski\cite{Kozminski91} noting the following:

\begin{quote}
It seems apparent that more benchmarks for layout synthesis are
needed, they should be more fully described, as discussed earlier in
the paper. In the opinion of the author, distributing pure
geometrical data is no longer sufficient for meaningful benchmarking
of algorithms used in real-life applications to produce working chip
layouts. The circuit's function as well as its speed and timing
requirements should be supplied together with technological data
required to verify the performance of the completed design. As an
unavoidable consequence, future benchmarks will have to come from
sources willing to put the information about logical functions of the
benchmark circuits in the public domain.
\end{quote}

A decade later, however, the original MCNC
benchmarks were still in wide use, and
the gap between academic research and
industrial practice had widened.
The lack of new benchmarks resulted in many groups
adapting the MCNC benchmarks in one way or another.  Cell sizes
were adjusted and mapped to new libraries, to keep
track of changing process technologies.  Routing models
shifted from a channel-based approach to over-the-cell,
as additional routing layers became available.  The
files originally released were translated from one
format to another, to take advantage of new design
automation tools, and to provide compatibility with
new software.

All of these changes undermined one of the original objectives of the
benchmarks -- comparison of different placement strategies.  Rather
that having a clear indication of which techniques were effective, and
highlighting key problems to solve, the research literature was
cluttered with results that were unintentionally
ambiguous\cite{Madden010030}.

\subsection{From Partitioning to Placement}


Researchers in both academia and industry were troubled by a lack of
up-to-date physical benchmarks.  It was broadly understood that good
benchmarks were necessary for academic research -- but intellectual
property constraints limited what any leading-edge company could
release.  A benchmarking break-through for circuit placement arrived
in the form of the release of the ``ISPD98'' partitioning benchmarks by
Alpert\cite{Alpert980080}.

Alpert noted the following:
\begin{quote}
All information relating to circuit functionality, timing and
technology is removed. Unfortunately, this limits the direct
applicability of these circuits (e.g., functional replication for
partitioning); yet, the release of these circuits would have been
impossible otherwise.% \cite{Alpert980080}
\end{quote}

Intellectual property concerns were addressed by stripping
away functionality, and by removing some portions of the circuit.
With the net list structure, however, it was possible to construct
a placement benchmark that could be used for an objective such
as wire length minimization -- Wang\cite{Wang000260} did this
for standard cells, with Adya\cite{Adya020012} taking the idea
further to create placement benchmarks that mixed
standard cells and macro blocks.

% industry practice\cite{Adya040472}.

After a number of years of little change, there was a burst of
activity.  The GSRC supported GTX project\cite{Caldwell000693}
provided an excellent platform to work from, with many researchers
from academia and industry becoming involved.  The GSRC
``bookshelf\cite{bookshelf}'' became stocked with a variety
of design benchmarks and design automation tools.

% ISPD98 partitioning benchmarks by Chuck

% Partition to placement
% \cite{Wang000260}
% Converted to mixed size placement in ISPD02 by Igor
% \cite{Adya020012}
% Mixed size placer PHM in ISPD04
% \cite{Khatkhate040084}
% ISPD05 -- Adaptec and Bigblue, fixed macros
% \cite{Nam050216}


% \subsection{Bookshelf File Formats}



% SIGDA newsletter! Franc Brglez from MCNC
% http://web.cs.ucla.edu/classes/layout/testing/Examples.iscas/ISCAS89/Benchmark.readme

% {\bf Talk about production file formats versus things that
%   are easy to parse.  Decisions made for GSRC Bookshelf.}

% \cite{Caldwell000693}\cite{umichbookshelf}




\section{The ISPD Contests}

% \subsection{Growing the Research Community}
\iffalse


While the most obvious objective for releasing a benchmark for some
problem $X$ is to be able to find better solutions for problem $X$,
there are other objectives as well.  Good benchmarks, particularly
ones that simplify a complex problem, provide a training ground for
students entering a field.

The technological advances that
have enabled the growth of the semiconductor industry
are due to the hard work of researchers from around the
world.  Every seasoned expert was at some point in their
career a beginning student.

While the new placement benchmarks lacked the detail needed
to create a fully functional circuit, they were more than
adequate for a student to understand the constraints, and
to experiment with new ideas.  The simplified file formats
allowed for rapid prototyping.

There is often a trade-off:
a simple problem, with a single metric, versus
a complex ``realistic'' problem, with multiple competing
objectives.  Spanning tree, Steiner tree, moving to clock
tree, where we measure skew, source to sink delay, jitter,
power consumption.  To routing, where multiple trees compete
for resources.
Partitioning -- simple problem to measure, impact when used
in a complex tool is muted.  Single step of a design automation
flow, version a complete run.

Good benchmarks can be used to direct the focus of researchers
on critical problems.
\fi

\iffalse
Secondary objectives.  Training ground for new students
entering the field -- a way to build understanding.  Simplified
construction of tools, lower time and effort required to
experiment with new ideas.
\fi

% \subsection{Secondary Objectives}


With new ``anonymized'' net lists and simplified file formats,
many pieces were now in place to accelerate research
in physical design.  The physical design community, and
in particular those active within ISPD, collaborated to
create new benchmarks, and to have a forum for their dissemination.
This evolved into the ISPD contest series, with the first
contest focusing on circuit placement.

Typically, an ISPD contest involves the following.
\begin{itemize}
\item An industry-affiliated research group defines a contest
  problem, and associated metrics.  This is often aligned with
  a research challenge faced internally.
\item A handful of sample problems are provided - contestants
  use these for initial experiments and design tool development.
\item Prior to ISPD, the contestants provide executable versions
  of their tools to the industrial research group. The industrial
  group then
  performs experiments using both the public benchmarks, and
  also a set of benchmarks kept secret.
\item At ISPD, the results of the contest are announced, and all
  benchmarks are made publicly available.
\end{itemize}

This practice has been of tremendous benefit.  First, researchers with
access to leading-edge technology can reveal their most pressing
challenges, and make it possible for the broader community to
contribute -- while also protecting proprietary information.  The
problems faced are simplified so that new students can gain an
understanding of the field, and try new ideas and approaches in a
low-risk manner.  Comparisons, performed by the industrial group, are
balanced -- there is no tuning or cherry picking of results.  The
competition also helps build the research community; students from
different groups around the world have a shared experience, develop
friendships, and find potential research collaborators.


The ISPD contests are shown in Table \ref{tab:ispdcontest}.
In the remainder of this section, we will discuss placement-related
contests in more detail.

\begin{table}
  \caption{The ISPD Contests.}
  \begin{tabular}{|r|l|}\hline
    Year & Contest \\ \hline
    2005/2006 & Placement \\ \hline
    2007/2008 & Global Routing \\ \hline
    2009 & Clock Network Synthesis \\ \hline    
    2010 & High Performance Clock Network Synthesis \\ \hline    
    2011 & Routability-Driven Placement \\ \hline
    2012/2013 & Discrete Gate Sizing \\ \hline
    2014 & Detailed Routing-Driven Placement \\ \hline    
    2015 & Blockage-Aware Detailed Routing-Driven Placement \\ \hline
    2016 & Routability-Driven FPGA Placement \\ \hline
    2017 & Clock-Aware FPGA Placement \\ \hline    
    2018/2019 & Initial Detailed Routing \\ \hline    
    2020 & Wafer-Scale Deep Learning Accelerator Placement \\ \hline
    2021 & Wafer-Scale Physics Modeling \\ \hline
  \end{tabular}
  \label{tab:ispdcontest}
\end{table}

\subsection{Mixed Size Placement, 2005, 2006}

The ISPD contests began with a focus on placement,
using net lists with a mixture of standard cells
and macro blocks\cite{ISPD05_contest}.  As most
placement tools were unable to handle movable
macro blocks, these were fixed in place for the
contest.

A new set of benchmarks -- derived from industrial
designs, but with fewer modifications than found
with the ISPD '98 partitioning benchmarks -- were released.
These benchmarks were several orders of magnitude
larger than the MCNC set, testing the scalability
of different algorithmic approaches.  The largest
benchmark, bigblue4, contained more than 2.1 million
movable objects, and more than eight thousand fixed
macros.  The benchmarks
also had a great deal of internal white space; the
chips had fixed outlines, and not all silicon area
was utilized.

The contest organizers also provided scripts to
check legality of solutions, ensuring that comparisons
would be unbiased.  For 2005, the only metric
considered was half-perimeter wire length; in 2006,
the contest was extended to consider routing congestion.
Nine teams competed in 2005; the next year, the number
of teams increased to ten.

\begin{table}
  \caption{ISPD 2005 Placement Benchmarks}
  \begin{tabular}{|c||r|r|r|r|}\hline
    Circuit & \# Objects & \# Movable & \# Fixed & Nets \\ \hline
adaptec1 & 211447 & 210904 & 543 & 221142  \\
adaptec2 & 255023 & 254457 & 566 & 266009  \\
adaptec3 & 451650 & 450927 & 723 & 466758  \\
adaptec4 & 496045 & 494716 & 1329 & 515951  \\
bigblue1 & 278164 & 277604 & 560 & 284479  \\
bigblue2 & 557866 & 534782 & 23084 & 577235  \\
bigblue3 & 1096812 & 1095519 & 1293 & 1123170  \\
bigblue4 & 2177353 & 2169183 & 8170 & 2229886  \\ \hline
\end{tabular}
\end{table}


\subsection{Routability, 2011}

\input natarajan.tex
Details, cite some papers.

\subsection{Detail Routing Driven Placement, 2014, 2015}

Continuing the evolution of placement benchmarking,
the contests in 2014 and 2015 examined the interactions
of detailed routing with placement closely\cite{Yutsis14,Bustany15}.

Design rules for deep submicron circuitry are quite complex;
spacing, alignment, and a variety of other factors make
fabricating the wiring at the cell level difficult.  The new
benchmarks tracked minimum spacing rules, end-of-line rules,
pin blockage, and more.

Because of the complexity, contest problems shifted from the
simplified GSRC Bookshelf formats to industry-standard LEF
and DEF; more challenging for the development of an academic
tool, but necessary to capture the constraints of leading edge
design.

Despite the difficulty of the problem, many teams competed,
with UWPlacer taking first in the contest in 2014\cite{Darav15}, and second in
2015\cite{Darav16}, while a team from NTU took first in 2015\cite{Huang15}.

\iffalse
% \cite{Darav14, Darav15, Huang15}

\cite{Yutsis14}
Details, cite some papers.
Detail Routing.
{\bf Ismail?}
\fi


\subsection{FPGA Placement 2016, 2017}

FPGA placement became the topic of ISPD Contest for 2016 and 2017. As
FPGA hardware technology advances and FPGA design sizes increase, the
core EDA algorithms including place and route face greater
challenges. Traditional academic FPGA placement study was primarily
based on benchmarks in VPR\cite{Betz97} era. From modern FPGA design
point of view, these benchmarks are relatively small and the
underlying FPGA architecture is over simplified. A new set of
benchmarks mapped to latest commercial FPGAs can draw attention from
academic research groups.

In ISPD 2016 Xilinx held Routability-Driven FPGA Placement Contest
\cite{sy1}. This is the first FPGA placement congest aiming on routability,
with benchmarks mapped on state-of-the-art commercial FPGA
architecture based on 20nm technology. The contest attracted 19
academic teams, 12 of them submitted final placer into
benchmarking. Contesters' placers were tested on a set of 12
testcases, with largest size at 1.1 million movable instances. The
placement quality is measured by Vivado router in Xilinx tool flow. If
the placement passes legality check and it is routed successfully,
total routed wirelength is used as the key metric. The total runtime
of place/route also matters. It serves as a scaling factor impacting
the metric by +/-10 percent.  UTPlacer\cite{sy2}  (from University of Texas
Austin), RippleFPGA\cite{sy3}  (from Chinese University of Hong Kong) and
GPlace\cite{sy4}  (from University of Guelph) won the first, second and third
place, respectively.

ISPD 2017 contest, Clock Aware FPGA Placement Contest\cite{sy5} , is an
extension from 2016â€™s topic. Clocking network in FPGA device poses a
major constraint to placement problem, because FPGA architecture
designers need to find a balance between reducing clock network area
and giving place/route flexibility. The problem becomes very
challenging for modern FPGA designs with many clocks. 9 teams
submitted their placers to compete. The final winners are UTPlaceF 2.0
\cite{sy6} (from University of Texas Austin), NTUfplace\cite{sy7} (from National
Taiwan University) and RippleFPGA\cite{sy8}  (from Chinese University of Hong
Kong).

Two ISPD FPGA placement contests triggered a lot of academic research
in the subsequent years. Benchmarks released in the contests, together
with the industrial tool supported evaluation flow, set up a perfect
platform for researchers to try out various placement
algorithms. There are successes on better placement quality\cite{sy9} ,
faster runtime, algorithm parallelization\cite{sy10,sy11} , machine-learning
enhancement\cite{sy12}.



\iffalse
\subsection{Deep Learning Accelerator Placement, 2020}

Include this?  Not really placement, but placement-ish?
\fi


\section{From Benchmarks to Flows}

In the past few years, there have been increasing
numbers of open-source circuit designs,
providing a source 
for new benchmarks, and a way to evaluate entire
RTL-to-GDS flows.  Robust software frameworks for
parsing industrial file formats are available,
and there are open-source tools for most steps
in a flow.

\iffalse

Striking a balance between objectives.  Industry groups
would certainly like a production-ready tool, but this is
something that a small team of graduate students can't
build quickly.

Need to capture the essence of a problem, while keeping it
tractable.  Make the benchmark hard to game, so that solutions
actually resemble what we might want ``in practice.''

Trade-off on using library for parsing -- locks into a build
system, sometimes a language and set of tools.  Simple
file formats, by contrast, may lose the essential elements
that matter for an industrial design.

Evaluators, with painstaking detail, are important.

Complex tool flows for evaluation are trouble.

\fi


% \subsection{The OpenROAD}

On significant effort is the OpenROAD\cite{Ajayi19} project,
working to create a complete set of tools for RTL-to-GDS.

% Section on RDF
\input jinwook.tex


\section{Concluding Comments}

Certainly, much has changed over the years.  Preas\cite{Preas87} noted
in 1987 that some circuits with over 10,000 cells were becoming
commonplace -- challenging to handle because representing them ``may
require a substantial fraction of a megabyte for storage.''
Gaining access to benchmarks, even if they
were ``publicly available,'' required knowing who to ask, where to
look, and the navigation of anonymous FTP servers. Often, data
was exchanged with magnetic tapes sent through the physical mail.
Disk space, processor cycles, and memory, were all in short supply.

In this era, there are pocket-sized processors with
vast amounts of RAM, high speed wireless communications, and nearly
infinite storage and compute resources in the cloud.  The 2021
edition of the International Symposium on Physical Design will
be conducted virtually, with talks and events streaming to
attendees around the world.  This is possible {\em because} of
the decades of effort in the design automation community,
and the collaborations of countless researchers in academia and
industry.

Looking back, the changes are staggering.  It is safe to assume
that progress will continue, and the world of twenty, thirty,
forty years from now is hard to imagine.  Certainly, there
will be a need for new benchmarks, new metrics, new problem
definitions; we would hope that the ISPD contests would
continue to be a venue for framing the problems to solve,
and an entry point for future generations of design automation
researchers.

\iffalse
The research field has matured, and the community has grown
significantly.  For physical design, the contests held annually
at ISPD have been of tremendous benefit -- new benchmarks, organized
and curated by industry leaders, are made available.  These
new benchmarks track the leading technological edge, and help
guide the academic community towards the most pressing problems.
These benchmarks are often simplified, making an ideal platform
for graduate students to gain understanding and insight, without
becoming overwhelmed.

With the availability of complex circuit designs and tools, all in
open-source format, benchmarking of an entire design flow is possible --
and this is becoming more common.  Keeping some focus on simplified
problems at individual stages of a flow remains useful; we 
anticipate a range of benchmarks and metrics to be relevant over
the years to come.
\fi


\balance
\bibliographystyle{unsrt}
\bibliography{unify,stephen}


\end{document}

